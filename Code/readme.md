# üß† Causality Game: Benchmarking Causal Inference Agents

Causality Game is a benchmarking framework designed to evaluate and compare the performance, strategies, and capabilities of causal inference agents. It provides a standardized simulation environment where agents interact with structural causal models (SCMs) through an environment, gather data, and attempt to solve various causal inference tasks.

The framework enables:

- Controlled and repeatable experiments
- Fair agent comparison
- Customizable game instances, agents, and evaluation metrics
- Multiple mission types for different causal inference tasks

# üöÄ Getting Started

## üì¶ Installation

Install in editable mode (recommended for development):

```bash
git clone <your-repo-url>
cd causalitygame
pip install -e .
```

## üß© Core Components

To test an agent using the Causality Game, several components must be set up:

### 1. Structural Causal Models (SCMs)

üìç `causalitygame/scm`

SCMs are the foundational components that define the causal relationships between variables. The framework supports various types of SCMs:

- **Equation-based SCMs**: Symbolic equations defining causal relationships
- **Database-driven SCMs**: SCMs constructed from real-world datasets
- **Bayesian Network SCMs**: SCMs based on Bayesian networks
- **Physics-based SCMs**: Domain-specific SCMs modeling physical laws

> **Note**: There is no limitation on the creation of new SCMs.

Key features:

- Support for both numerical and categorical variables
- Customizable noise distributions
- Intervention capabilities for controllable variables
- Domain-specific node implementations

### 2. Missions

üìç `causalitygame/missions`

Missions define the specific causal inference task that agents need to solve:

- **DAG Inference Mission**: Learn the underlying causal structure (DAG)
- **Average Treatment Effect (ATE) Mission**: Estimate the average treatment effect
- **Conditional Average Treatment Effect (CATE) Mission**: Estimate treatment effects conditional on covariates
- **Treatment Effect (TE) Mission**: General treatment effect estimation

> **Note**: There is no limitation on the creation of new missions.

### 3. Game Instances

üìç `causalitygame/game_engine.GameInstance`

Game instances encapsulate all necessary components for a benchmark run:

- A specific SCM
- A mission with defined metrics
- Game parameters (max rounds, random state)
- Serialization capabilities for reproducibility

### 4. Agents

üìç `causalitygame/agents`

Agents are the players that interact with the environment to solve missions:

Built-in agents:

- **RandomAgent**: Makes random decisions about experiments and stopping
- **ExhaustiveAgent**: Systematically explores all possible interventions

Custom agents can be implemented by extending [BaseAgent](file:///Users/sergioamortegui/Desktop/Business/Research/Causality/Code/causalitygame/agents/abstract.py#L7-L48) in [causalitygame/agents/abstract.py](file:///Users/sergioamortegui/Desktop/Business/Research/Causality/Code/causalitygame/agents/abstract.py).

> **Note**: There is no limitation on the creation of new agents.

### 5. Evaluation Metrics

üìç `causalitygame/evaluators`

Metrics evaluate both agent behavior and deliverables:

**Behavior Metrics** (evaluate agent conduct):

- [ExperimentsBehaviorMetric](file:///Users/sergioamortegui/Desktop/Business/Research/Causality/Code/causalitygame/evaluators/behavior/ExperimentsBehaviorMetric.py#L0-L0): Number of experiments performed
- [RoundsBehaviorMetric](file:///Users/sergioamortegui/Desktop/Business/Research/Causality/Code/causalitygame/evaluators/behavior/RoundsBehaviorMetric.py#L0-L0): Number of rounds taken
- [TreatmentsBehaviorMetric](file:///Users/sergioamortegui/Desktop/Business/Research/Causality/Code/causalitygame/evaluators/behavior/TreatmentsBehaviorMetric.py#L0-L0): Number of treatments applied

**Deliverable Metrics** (evaluate solution quality):

- [AbsoluteErrorDeliverableMetric](file:///Users/sergioamortegui/Desktop/Business/Research/Causality/Code/causalitygame/evaluators/deliverable/AbsoluteErrorDeliverableMetric.py#L0-L0): Absolute error in estimations
- [EdgeAccuracyDeliverableMetric](file:///Users/sergioamortegui/Desktop/Business/Research/Causality/Code/causalitygame/evaluators/deliverable/EdgeAccuracyDeliverableMetric.py#L0-L0): Accuracy of learned graph edges
- [F1DeliverableMetric](file:///Users/sergioamortegui/Desktop/Business/Research/Causality/Code/causalitygame/evaluators/deliverable/F1DeliverableMetric.py#L0-L0): F1 score for graph recovery
- [MeanSquaredErrorDeliverableMetric](file:///Users/sergioamortegui/Desktop/Business/Research/Causality/Code/causalitygame/evaluators/deliverable/MeanSquaredErrorDeliverableMetric.py#L0-L0): MSE of estimations
- [SHDDeliverableMetric](file:///Users/sergioamortegui/Desktop/Business/Research/Causality/Code/causalitygame/evaluators/deliverable/SHDDeliverableMetric.py#L0-L0): Structural Hamming Distance for graph recovery

> **Note**: There is no limitation on the creation of new metrics.

### 6. Game Runner

üìç `causalitygame/game_engine.Game`

The central class that orchestrates the benchmark:

- Loads game instances
- Runs each agent in identical environments
- Applies evaluation metrics
- Produces results and visualizations

## üß™ Example: Running the Benchmark

### Define Agents

```python
import causalitygame as cg
from causalitygame.agents.random import RandomAgent
from causalitygame.agents.exhaustive import ExhaustiveAgent

# Define the agents
agents = [
    (f"Random Agent {i}", RandomAgent(seed=911 + i, samples_range=(1, 3)))
    for i in range(1, 3)
]
# Add an exhaustive agent
agents.append(("Exhaustive Agent", ExhaustiveAgent(num_obs=1)))
```

### Define Game Instance

```python
# Game Instance
game_instance_path = "causalitygame/data/game_instances/cate/foundations_instance.json"
```

### Define Metrics (Optional)

```python
from causalitygame.evaluators.behavior import (
    ExperimentsBehaviorMetric, TreatmentsBehaviorMetric, RoundsBehaviorMetric
)
from causalitygame.evaluators.deliverable import (
    SHDDeliverableMetric, F1DeliverableMetric, EdgeAccuracyDeliverableMetric
)

behavior_metrics = [
    ExperimentsBehaviorMetric(),
    TreatmentsBehaviorMetric(),
    RoundsBehaviorMetric(),
]

deliverable_metrics = [
    SHDDeliverableMetric(),
    F1DeliverableMetric(),
    EdgeAccuracyDeliverableMetric(),
]
```

### Define Custom Hooks (Optional)

```python
def on_game_start():
    ...


def on_agent_game_start(agent_name):
    ...


def on_round_start(agent_name, round, state, actions, samples):
    ...


def on_action_chosen(agent_name, state, action, action_object):
    ...


def on_action_evaluated(agent_name, state, action, action_object, result):
    ...


def on_round_end(agent_name, round, state, action, action_object, samples, result):
    ...


def on_agent_game_end(agent_name):
    ...


def on_game_end():
    ...
```

### Run Game

```python
# Game
import pandas as pd
import causalitygame as cg

# Plotting
import matplotlib.pyplot as plt

# Agents
from causalitygame.agents.random import RandomAgent
from causalitygame.agents.exhaustive import ExhaustiveAgent


# Define the agents
agents = [
    (f"Random Agent {i}", RandomAgent(seed=911 + i, samples_range=(1, 3)))
    for i in range(1, 3)
]
# Add an exhaustive agent
agents.append(("Exhaustive Agent", ExhaustiveAgent(num_obs=1)))
# Game Instance
game_instance_path = "causalitygame/data/game_instances/cate/foundations_instance.json"


# Data for plotting
data = {}

datasets = {}


# Hooks
def on_game_start():
    ...


def on_agent_game_start(agent_name):
    ...


def on_round_start(agent_name, round, state, actions, samples):
    ...


def on_action_chosen(agent_name, state, action, action_object):
    ...


def on_action_evaluated(agent_name, state, action, action_object, result):
    ...


def on_round_end(agent_name, round, state, action, action_object, samples, result):
    ...


def on_agent_game_end(agent_name):
    ...


def on_game_end():
    ...


# Create a game
game = cg.Game(
    agents=agents,
    game_spec=game_instance_path,
    behavior_metrics=behavior_metrics, # Optional 
    deliverable_metrics=deliverable_metrics, # Optional 
    hooks={ # Optional 
        "on_game_start": on_game_start,
        "on_agent_game_start": on_agent_game_start,
        "on_round_start": on_round_start,
        "on_action_chosen": on_action_chosen,
        "on_action_evaluated": on_action_evaluated,
        "on_round_end": on_round_end,
        "on_agent_game_end": on_agent_game_end,
        "on_game_end": on_game_end,
    },
    seed=911, # Optional seed for reproducibility
)
# Run the game
runs = game.run()

# Print the results
game.plot()
```

### Simple Example

```python
import causalitygame as cg
from causalitygame.agents.random import RandomAgent
from causalitygame.agents.exhaustive import ExhaustiveAgent

# Define the agents
agents = [(f"Random Agent {i}", RandomAgent(seed=911 + i)) for i in range(1, 3)]
# Add an exhaustive agent
agents.append(("Exhaustive Agent", ExhaustiveAgent()))

# Game Instance
game_instance_path = "causalitygame/data/game_instances/dag_inference/ideal_gas_instance.json"

# Create a game
game = cg.Game(agents=agents, game_spec=game_instance_path)

# Run the game
runs = game.run()

# Print the results
game.plot()
```

### üìä Output

After running `game.run()`:

- The result is a dictionary with agent names as keys.
- Each entry contains:
  - "history": Full action log (experiments, datasets)
  - "raw": Raw metric values
  - "behavior_score": Weighted behavior score
  - "deliverable_score": Weighted deliverable score

### üñºÔ∏è Visualize Performance

```python
game.plot()
```

Creates a scatter plot where:

- X-axis: behavior score
- Y-axis: deliverable score
- Each point = one agent

This makes it easy to compare agent strategies and outcomes.

## üìÅ Project Structure

```
causalitygame/
‚îÇ
‚îú‚îÄ‚îÄ agents/                 # Agent implementations
‚îÇ   ‚îú‚îÄ‚îÄ abstract.py         # Base agent class
‚îÇ   ‚îú‚îÄ‚îÄ random.py           # Random agent implementation
‚îÇ   ‚îî‚îÄ‚îÄ exhaustive.py       # Exhaustive agent implementation
‚îÇ
‚îú‚îÄ‚îÄ data/                   # Data resources
‚îÇ   ‚îú‚îÄ‚îÄ datasets/           # Real-world datasets
‚îÇ   ‚îú‚îÄ‚îÄ game_instances/     # Predefined game instances
‚îÇ   ‚îî‚îÄ‚îÄ scms/               # Structural causal models
‚îÇ
‚îú‚îÄ‚îÄ evaluators/             # Evaluation metrics
‚îÇ   ‚îú‚îÄ‚îÄ behavior/           # Behavior metrics
‚îÇ   ‚îú‚îÄ‚îÄ deliverable/        # Deliverable metrics
‚îÇ   ‚îú‚îÄ‚îÄ abstract.py         # Base metric classes
‚îÇ   ‚îî‚îÄ‚îÄ Evaluator.py        # Evaluation orchestrator
‚îÇ
‚îú‚îÄ‚îÄ game_engine/            # Core game components
‚îÇ   ‚îú‚îÄ‚îÄ Game.py             # Main game runner
‚îÇ   ‚îú‚îÄ‚îÄ GameInstance.py     # Game instance definitions
‚îÇ   ‚îú‚îÄ‚îÄ Environment.py      # Game environment
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ
‚îú‚îÄ‚îÄ generators/             # Component generators
‚îÇ   ‚îú‚îÄ‚îÄ outcome/            # Outcome generators
‚îÇ   ‚îú‚îÄ‚îÄ abstract.py         # Base generator classes
‚îÇ   ‚îú‚îÄ‚îÄ dag_generator.py    # DAG generators
‚îÇ   ‚îî‚îÄ‚îÄ scm_generator.py    # SCM generators
‚îÇ
‚îú‚îÄ‚îÄ lib/                    # Utilities and helpers
‚îÇ   ‚îú‚îÄ‚îÄ constants/          # Constant definitions
‚îÇ   ‚îú‚îÄ‚îÄ helpers/            # Helper functions
‚îÇ   ‚îú‚îÄ‚îÄ scripts/            # External algorithm implementations
‚îÇ   ‚îî‚îÄ‚îÄ utils/              # Utility functions
‚îÇ
‚îú‚îÄ‚îÄ missions/               # Mission definitions
‚îÇ   ‚îú‚îÄ‚îÄ abstract.py         # Base mission class
‚îÇ   ‚îú‚îÄ‚îÄ DAGInferenceMission.py
‚îÇ   ‚îú‚îÄ‚îÄ AverageTreatmentEffectMission.py
‚îÇ   ‚îú‚îÄ‚îÄ ConditionalAverageTreatmentEffectMission.py
‚îÇ   ‚îî‚îÄ‚îÄ TreatmentEffectMission.py
‚îÇ
‚îú‚îÄ‚îÄ scm/                    # Structural Causal Models
‚îÇ   ‚îú‚îÄ‚îÄ dags/               # DAG implementations
‚îÇ   ‚îú‚îÄ‚îÄ impl/               # SCM implementations
‚îÇ   ‚îú‚îÄ‚îÄ nodes/              # Node definitions
‚îÇ   ‚îú‚îÄ‚îÄ noises/             # Noise distributions
‚îÇ   ‚îú‚îÄ‚îÄ abstract.py         # Base SCM classes
‚îÇ   ‚îî‚îÄ‚îÄ db.py               # Database-driven SCM
‚îÇ
‚îú‚îÄ‚îÄ translators/            # Format translators
‚îÇ   ‚îî‚îÄ‚îÄ bif_translator.py   # BIF format translator
‚îÇ
‚îî‚îÄ‚îÄ __init__.py
```

## üìö Key Features

1. **Modular Design**: Each component is loosely coupled, allowing easy extension and customization
2. **Reproducibility**: Game instances can be serialized and shared for consistent benchmarking
3. **Multiple Mission Types**: Support for various causal inference tasks beyond just DAG learning
4. **Extensible Architecture**: Easy to add new agents, metrics, SCMs, and missions
5. **Real-world Datasets**: Integration with standard causal inference datasets
6. **Physics-based SCMs**: Domain-specific SCMs modeling physical laws
7. **Comprehensive Evaluation**: Both behavior and deliverable metrics for holistic assessment

## üõ†Ô∏è Development

To contribute to the project:

1. Fork the repository
2. Create a feature branch
3. Implement your changes
4. Add tests if applicable
5. Submit a pull request

Run tests with:

```bash
pytest
```

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.
