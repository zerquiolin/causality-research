```
src/
â”‚
â”œâ”€â”€ benchmark/             # ğŸ’¡ High-level orchestration & benchmark runner
â”‚   â”œâ”€â”€ runner.py          # Orchestrates full benchmark lifecycle
â”‚   â”œâ”€â”€ config/            # Config loading, validation (YAML/JSON schemas)
â”‚   â”‚   â””â”€â”€ schema.py
â”‚   â””â”€â”€ registry.py        # For dynamic component loading
â”‚
â”œâ”€â”€ environments/          # ğŸŒ Environments (base + implementations)
â”‚   â”œâ”€â”€ base.py
â”‚   â”œâ”€â”€ metrics.py         # Tracking, evaluations, scoring
â”‚   â””â”€â”€ impl/              # Concrete envs
â”‚       â””â”€â”€ my_env.py
â”‚
â”œâ”€â”€ scm/                   # âš™ï¸ Structural Causal Models
â”‚   â”œâ”€â”€ base.py            # Abstract SCM logic
â”‚   â”œâ”€â”€ dag.py             # DAG structure + utils
â”‚   â”œâ”€â”€ nodes.py           # SCM nodes (atomic units)
â”‚   â””â”€â”€ impl/              # Different SCM implementations
â”‚       â””â”€â”€ my_scm.py
â”‚
â”œâ”€â”€ agents/                # ğŸ§  Agents that interact with the env
â”‚   â”œâ”€â”€ base.py
â”‚   â””â”€â”€ impl/              # Concrete agent types
â”‚       â””â”€â”€ greedy_agent.py
â”‚
â”œâ”€â”€ generators/            # ğŸ—ï¸ DAG/SCM/Env generators (can be heavy)
â”‚   â”œâ”€â”€ base.py            # Optional base class if needed
â”‚   â”œâ”€â”€ dag_generator.py
â”‚   â”œâ”€â”€ scm_generator.py
â”‚   â””â”€â”€ env_generator.py
â”‚
â”œâ”€â”€ evaluation/            # ğŸ“Š Metrics, analysis, plots
â”‚   â”œâ”€â”€ evaluator.py       # Evaluation logic
â”‚   â””â”€â”€ visualizer.py      # Optional: plots/stats (matplotlib, seaborn)
â”‚
â”œâ”€â”€ lib/                   # ğŸ› ï¸ Shared utilities
â”‚   â”œâ”€â”€ constants.py
â”‚   â”œâ”€â”€ utils/             # General-purpose helper functions
â”‚   â”‚   â””â”€â”€ file_utils.py
â”‚   â””â”€â”€ models/            # Abstract models (shared interfaces)
â”‚       â””â”€â”€ abstract/
â”‚           â”œâ”€â”€ agent.py
â”‚           â”œâ”€â”€ scm.py
â”‚           â””â”€â”€ environment.py
â”‚
tests/                     # âœ… Unit and integration tests
â”‚   â”œâ”€â”€ environments/
â”‚   â”œâ”€â”€ scm/
â”‚   â”œâ”€â”€ agents/
â”‚   â””â”€â”€ ...
```

# ğŸ§  Causality Game: Benchmarking Causal Inference Agents

Causality Game is a benchmarking framework designed to evaluate and compare the performance, strategies, and capabilities of causal discovery agents. It provides a standardized simulation environment where agents interact with structural causal models (SCMs), gather data, and attempt to learn underlying causal graphs.

The game enables:
 â€¢ Controlled and repeatable experiments
 â€¢ Fair agent comparison
 â€¢ Customizable game instances, agents, and evaluation metrics

# ğŸš€ Getting Started

## ğŸ“¦ Installation

Install in editable mode (recommended for development):

git clone <your-repo-url>
cd causalitygame
pip install -e .

## ğŸ§© Main Components

To test an agent using the Causality Game, several components must be set up:

### 1. DAG Generator

ğŸ“ causalitygame.generators.dag_generator

Use this module to generate directed acyclic graphs (DAGs) that define the structure of your SCM.
 â€¢ Supports pre-built DAG generation functions (e.g., binary tree, ER graphs).
 â€¢ You can implement your own by extending the base DAG class from causalitygame.scm.base.

### 2. SCM Generator

ğŸ“ causalitygame.generators.scm_generator

Generates a Structural Causal Model (SCM) over a given DAG:
 â€¢ Handles both numerical and categorical variables.
 â€¢ Creates symbolic equations and sampling noise.
 â€¢ Includes support for defining CDFs for categorical variables.

Custom SCM implementations should extend:
 â€¢ SCM class (for the full model)
 â€¢ SCMNode class (for individual variables) from causalitygame.scm.base.

### 3. Game Instance Generator

ğŸ“ causalitygame.game.GameInstance

Wraps together the DAG, SCM, and random state into a full game-ready configuration.
 â€¢ Use .save(filename) to serialize a game instance.
 â€¢ Reload with GameInstance.from_dict(...) or via joblib.load(...).

This ensures each agent plays under identical conditions.

### 4. Agents (Players)

ğŸ“ causalitygame.agents.base and causalitygame.agents.impl

Agents are the players in the game. They decide:
 â€¢ When to stop exploring
 â€¢ What interventions or experiments to perform
 â€¢ How to construct their estimated causal graph

You can:
 â€¢ Create a custom agent by inheriting from BaseAgent in agents.base.
 â€¢ Use built-in agents like RandomAgent and ExhaustiveAgent from agents.impl.

### 5. Metrics

ğŸ“ causalitygame.evaluators.impl and causalitygame.evaluators.base

Metrics are used to evaluate an agentâ€™s behavior and results.

Two types:
 â€¢ Behavior Metrics: e.g. number of experiments, number of samples used.
 â€¢ Deliverable Metrics: e.g. SHD, F1-score, edge accuracy.

To define your own, extend the base classes in:
 â€¢ causalitygame.evaluators.base.BehaviorMetric
 â€¢ causalitygame.evaluators.base.DeliverableMetric

### 6. Game Runner

ğŸ“ causalitygame.game.Game

The central class that:
 â€¢ Loads a game instance
 â€¢ Runs each agent in identical environments
 â€¢ Applies evaluation metrics
 â€¢ Produces output reports and plots

Constructor parameters:
 â€¢ agents: list of (name, agent_instance)
 â€¢ game_spec: path to a saved game instance (.pkl)
 â€¢ behavior_metrics: list of instantiated behavior metric objects
 â€¢ deliverable_metrics: list of instantiated deliverable metric objects
 â€¢ seed: optional configuration

## ğŸ§ª Example: Running the Benchmark

### Agents

```python
from causalitygame.agents.impl.RandomAgent import RandomAgent
from causalitygame.agents.impl.ExhaustiveAgent import ExhaustiveAgent
```

### Metrics

```python
from causalitygame.evaluators.impl.behavior import (
    ExperimentsBehaviorMetric, TreatmentsBehaviorMetric, RoundsBehaviorMetric
)
from causalitygame.evaluators.impl.deliverable import (
    SHDDeliverableMetric, F1DeliverableMetric, EdgeAccuracyDeliverableMetric
)
```

### Game

```python
from causalitygame.game.Game import Game
import numpy as np

base_seed = 42
agents = []

for i in range(1, 6):
    rs = np.random.RandomState(base_seed + i)
    agent = (
        f"random {i}",
        RandomAgent(
            stop_probability=rs.beta(a=0.5, b=10),
            experiments_range=(1, max(rs.poisson(10), 2)),
            samples_range=(rs.randint(500, 800), rs.randint(800, 1000)),
            seed=base_seed + i,
        ),
    )
    agents.append(agent)

agents.append(("exhaustive", ExhaustiveAgent()))

behavior_metrics = [
    ExperimentsBehaviorMetric(),
    TreatmentsBehaviorMetric(),
    RoundsBehaviorMetric(),
]

deliverable_metrics = [
    SHDDeliverableMetric(),
    F1DeliverableMetric(),
    EdgeAccuracyDeliverableMetric(),
]

game = Game(
    agents=agents,
    game_spec="instances/game_instance.pkl",  # path to your saved GameInstance
    behavior_metrics=behavior_metrics,
    deliverable_metrics=deliverable_metrics,
    seed=911,
)


results = game.run()
print(results)
game.plot()
```

### ğŸ“Š Output

After running game.run():

- The result is a dictionary with agent names as keys.
- Each entry contains:
- "history": Full action log (experiments, datasets)
- "raw": Raw metric values
- "behavior_score": Weighted behavior score
- "deliverable_score": Weighted deliverable score

ğŸ–¼ï¸ Visualize Performance

game.plot()

Creates a scatter plot where:
 â€¢ X-axis: behavior score
 â€¢ Y-axis: deliverable score
 â€¢ Each point = one agent

This makes it easy to compare agent strategies and outcomes.

## ğŸ“š Summary

Component Description:

- dag_generator: Generate or customize DAG structures.
- scm_generator: Create symbolic SCM equations over DAGs.
- GameInstance: Bundle DAG + SCM for reproducible game.
- agents.impl: Ready-made agents (Random, Exhaustive).
- agents.base: Interface for custom agent design.
- evaluators.impl: Built-in behavior & deliverable metrics.
- evaluators.base: For building your own metrics.
- Game: Runs the full benchmark and evaluation.

Let me know if youâ€™d like this turned into a README.md or integrated into docstrings across the repo.
